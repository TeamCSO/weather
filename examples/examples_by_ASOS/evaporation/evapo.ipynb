{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import h2o\n",
    "import array\n",
    "import numpy as np\n",
    "import re \n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from h2o.estimators.glm import H2OGeneralizedLinearEstimator\n",
    "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
    "from h2o.estimators.random_forest import H2ORandomForestEstimator\n",
    "from h2o.estimators import H2ORandomForestEstimator\n",
    "from h2o.grid.grid_search import H2OGridSearch\n",
    "from missingpy import KNNImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#============================================================\n",
    "# 분석 환경 셋팅\n",
    "#============================================================\n",
    "sys.stdout.flush() #Python 메모리에 생성된 모든 객체 삭제(초기화)\n",
    "\n",
    "#============================================================\n",
    "# 작업 디렉토리 경로 확인\n",
    "#============================================================\n",
    "currentPath=os.getcwd() #현재 위치한 디렉토리 경로확인\n",
    "print('Current working dir : %s' % currentPath)\n",
    "\n",
    "#============================================================\n",
    "# 기상 데이터 읽어오기 \n",
    "#============================================================\n",
    "ASOS_DATA = pd.read_csv(currentPath + '/ASOS_DATA.csv', encoding='euc-kr') #loading  weather data\n",
    "GEO_DATA = pd.read_csv(currentPath + '/GEO_DATA.csv', encoding='euc-kr') #loading geographical data\n",
    "#============================================================\n",
    "# 불러온 데이터 구조 확인하기 \n",
    "#============================================================ \n",
    "ASOS_DATA.info()\n",
    "GEO_DATA.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#=============================================================\n",
    "# 필요한 컬럼만 추출하기 \n",
    "#============================================================= \n",
    "GEO_DATA = GEO_DATA.loc[:, ['STN_ID', 'TM_ED', 'TM_ST', 'HT', 'HT_WD']] # 전체 행 출력, 열이 'A' 또는 'B'인 데이터 출력\n",
    "\n",
    "#============================================================\n",
    "# TM_ED(지점종료날짜), TM_ST(지점시작날짜) 숫자 데이터 타입으로 변환\n",
    "#============================================================\n",
    "\n",
    "# 데이터 변환 및 추출\n",
    "#GEO_DATA['TM_ST'] = GEO_DATA['TM_ST'].str.replace('.', '')\n",
    "#GEO_DATA['TM_ED'] = GEO_DATA['TM_ED'].str.replace('.', '')\n",
    "GEO_DATA['TM_ST'] = GEO_DATA['TM_ST'].astype(str).str.replace(r'\D+','', regex=True)\n",
    "GEO_DATA['TM_ED'] = GEO_DATA['TM_ED'].astype(str).str.replace(r'\D+','', regex=True)\n",
    "\n",
    "GEO_DATA['TM_ST'] = GEO_DATA['TM_ST'].str.slice(0,8)\n",
    "GEO_DATA['TM_ED'] = GEO_DATA['TM_ED'].str.slice(0,8)\n",
    "\n",
    "# str을 int 로 변환\n",
    "GEO_DATA['TM_ST'] = GEO_DATA['TM_ST'].astype(int)\n",
    "GEO_DATA['TM_ED'] = GEO_DATA['TM_ED'].astype(int)\n",
    "\n",
    "# 테이터 타입 확인\n",
    "GEO_DATA.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#=============================================================\n",
    "# 기상데이터와 HT(관측지점높이) 결합 \n",
    "#=============================================================\n",
    "#146(전주)지점 제외하고 결합\n",
    "mergeGeo = GEO_DATA.loc[GEO_DATA['STN_ID'] != 146,['STN_ID','HT']]\n",
    "\n",
    "# STNID 컬럼으로 asos 데이터와 geo 데이터 병합\n",
    "DATA = pd.merge(ASOS_DATA, mergeGeo, how='left', on='STN_ID')\n",
    "\n",
    "#146(전주)지점 관측기간에 맞는 HT 데이터 결합하기\n",
    "DATA.at[(DATA['STN_ID'] == 146) & (DATA['TM'] < 20150701), 'HT'] = 53.40\n",
    "DATA.at[(DATA['STN_ID'] == 146) & (DATA['TM'] >= 20150701), 'HT'] = 61.40\n",
    "\n",
    "#결과 확인하기\n",
    "DATA.loc[(DATA['STN_ID'] == 146) & (DATA['TM'] < 20150701), ['STN_ID','TM','HT']].head()\n",
    "DATA.loc[(DATA['STN_ID'] == 146) & (DATA['TM'] >= 20150701), ['STN_ID','TM','HT']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#============================================================\n",
    "# 요약통계 한번에 보기 \n",
    "#============================================================\n",
    "DATA.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#============================================================\n",
    "# 지점별 요약통계 보기 \n",
    "#============================================================\n",
    "grouped = DATA.groupby('STN_ID')\n",
    "grouped.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#============================================================\n",
    "# STN별 박스플롯 그리기\n",
    "#============================================================\n",
    "sns.set()\n",
    "sns.set_style(\"ticks\")\n",
    "%matplotlib inline\n",
    "\n",
    "# boxplot\n",
    "for col in DATA.columns.difference(['STN_ID', 'TM']): \n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.title(\"Boxplot of \" + col + \" by STN_ID\")\n",
    "    sns.boxplot(x=\"STN_ID\", y=col, data=DATA, palette=\"Set1\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#============================================================\n",
    "# Histogram 그리기 \n",
    "#============================================================\n",
    "for col in DATA.columns.difference(['STN_ID', 'TM']):\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.title(\"Histogram of \" + col)\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Freqeuncy')\n",
    "    plt.hist(DATA[col].dropna())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#=============================================================\n",
    "# 이상치 처리\n",
    "#=============================================================\n",
    "# 소형증발량 기후자료 품질검사 알고리즘 기준값: 0~15 -> NA로 치환\n",
    "DATA.at[(DATA['SUM_SML_EV'] > 15) | (DATA['SUM_SML_EV'] < 0), 'SUM_SML_EV'] = np.nan\n",
    "DATA['SUM_SML_EV'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#============================================================\n",
    "# 결측치 처리 \n",
    "#============================================================\n",
    "# SUM_RN, SUM_SS, SD_HR3_MAX, SD_TOT_MAX는 0으로 치환 \n",
    "DATA['SUM_RN'] = DATA['SUM_RN'].fillna(value = 0)\n",
    "DATA['SUM_SS'] = DATA['SUM_SS'].fillna(value = 0)\n",
    "DATA['SD_HR3_MAX'] = DATA['SD_HR3_MAX'].fillna(value = 0)\n",
    "DATA['SD_TOT_MAX'] = DATA['SD_TOT_MAX'].fillna(value = 0)\n",
    "DATA.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SUM_SI, SUM_SML_EV는 KnnImputation을 활용하여 결측치 처리 \n",
    "imputer = KNNImputer(n_neighbors=10)\n",
    "DATA = pd.DataFrame(imputer.fit_transform(DATA), columns=DATA.columns)\n",
    "DATA.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#============================================================\n",
    "# 파생변수 생성 \n",
    "#============================================================\n",
    "# 강수유무 생성\n",
    "DATA['RAIN'] = np.nan\n",
    "DATA.at[(DATA['SUM_RN'] == 0), 'RAIN'] = 0\n",
    "DATA.at[(DATA['SUM_RN'] > 0), 'RAIN'] = 1\n",
    "DATA.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2m 풍속 생성\n",
    "# 수식 : 일평균풍속 * (4.87/ln(67.8*풍속계높이 - 5.42))\n",
    "# 데이터에 HT_WD(풍속계높이) 컬럼 결합하기\n",
    "mergeAll = GEO_DATA.loc[GEO_DATA['STN_ID'] != 146,['STN_ID','HT_WD']]\n",
    "\n",
    "# STNID 컬럼으로 asos 데이터와 geo 데이터 병합\n",
    "DATA = pd.merge(DATA, mergeAll, how='left', on='STN_ID')\n",
    "\n",
    "DATA.at[(DATA['STN_ID'] == 146) & (DATA['TM'] < 20150701), 'HT_WD'] = 18.4\n",
    "DATA.at[(DATA['STN_ID'] == 146) & (DATA['TM'] >= 20150701), 'HT_WD'] = 10.0\n",
    "DATA.describe()\n",
    "\n",
    "# 2m일평균풍속 계산하기\n",
    "DATA['WS_2m'] = DATA['WS_AVG'] * (4.87 / (np.log((67.8 * DATA['HT_WD']) - 5.42)))\n",
    "\n",
    "# 풍속계높이 변수 삭제\n",
    "DATA.drop('HT_WD', axis=1, inplace=True)\n",
    "\n",
    "DATA.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #============================================================\n",
    "# # H2O 가상 서비 셋팅 \n",
    "# #============================================================\n",
    "h2o.init(max_mem_size = \"4G\", nthreads = 4)\n",
    "h2o.remove_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#============================================================\n",
    "# 변수들간 다중공선성 제거 \n",
    "#============================================================\n",
    "# 상관계수 구하기\n",
    "corr_data = DATA.drop(['STN_ID', 'TM', 'SUM_SML_EV'], axis=1)\n",
    "\n",
    "corr=corr_data.corr()\n",
    "Abs_corr=abs(pd.DataFrame(corr))\n",
    "Abs_corr['variable']=Abs_corr.index\n",
    "Abs_corr.reset_index(drop=True, inplace=True)\n",
    "Abs_corr.head()\n",
    "\n",
    "# variable importance 산출하기\n",
    "data_hex = h2o.H2OFrame(DATA)\n",
    "new_data_hex = data_hex.drop(['STN_ID', 'TM', 'SUM_SML_EV'], axis=1)\n",
    "\n",
    "xList = new_data_hex.columns\n",
    "y = \"SUM_SML_EV\"\n",
    "\n",
    "fit = H2ORandomForestEstimator(ntrees=50, max_depth=20, seed=1)\n",
    "fit.train(x=xList, y=y, training_frame=data_hex)\n",
    "\n",
    "varimp = fit.varimp(True)\n",
    "varimp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 다중공선성 제거\n",
    "finalvar = varimp \n",
    "j=0\n",
    "\n",
    "while j < len(finalvar.variable):\n",
    "    tmp = Abs_corr[(Abs_corr.variable != 'y') & (Abs_corr.variable != finalvar.variable[j])].loc[:,['variable',finalvar.variable[j]]]\n",
    "    tmp.columns = ['variable', 'Pearson']\n",
    "    tmp.sort_values(['Pearson'], axis=0, ascending=False, inplace=True)\n",
    "    tmp = tmp[tmp.Pearson > 0.45]\n",
    "\n",
    "    finalvar = pd.merge(finalvar, tmp, how='left', on='variable')\n",
    "    finalvar = finalvar.loc[finalvar.isnull()['Pearson'], :]\n",
    "    finalvar = finalvar.reset_index(drop=True)\n",
    "    finalvar = finalvar.drop('Pearson', 1)\n",
    "    finalvar.sort_values(['scaled_importance'], axis=0, ascending=False, inplace=True)\n",
    "    j = j + 1\n",
    "\n",
    "finalvar #최종선택변수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#============================================================#\n",
    "#하이퍼파라미터 최적화\n",
    "#============================================================#\n",
    "# 최종선택변수\n",
    "varList = list(finalvar['variable'])\n",
    "y = \"SUM_SML_EV\"\n",
    "\n",
    "# 하이퍼파라미터 조합만들기\n",
    "hyper_params = {'sample_rate': [0.3, 0.4],\n",
    "                'max_depth': [18, 20, 25],\n",
    "                'ntrees': [25, 50],\n",
    "                'mtries': [-1, 1]}\n",
    "\n",
    "# 조합 모형 돌리기\n",
    "m = H2OGridSearch(H2ORandomForestEstimator, grid_id = 'rf_grid', hyper_params=hyper_params)\n",
    "m.train(x = varList, y = y, training_frame = data_hex)\n",
    "\n",
    "# mse가 낮은 순으로 정렬하기\n",
    "sorted_grid =m.get_grid(sort_by = 'mse')\n",
    "print(sorted_grid)\n",
    "\n",
    "# 베스트 모형 선택\n",
    "best_model = h2o.get_model(sorted_grid.model_ids[0])\n",
    "print(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#============================================================\n",
    "# 모형 성능 검증\n",
    "#============================================================\n",
    "# sMAPE 함수 정의\n",
    "def cal_smape_100(y, yhat):\n",
    "    return np.mean(abs((yhat - y))/(abs(yhat) + abs(y)))\n",
    "\n",
    "# MODEL RUNNING\n",
    "fit = H2ORandomForestEstimator(ntrees=50, max_depth=18, sample_rate = 0.3, mtries = -1, seed=1)\n",
    "fit.train(x = varList, y = y, training_frame = data_hex)\n",
    "\n",
    "# result\n",
    "perform = pd.DataFrame({'mse':[fit.mse()], 'r2':[fit.r2()]})\n",
    "varimp = pd.DataFrame(fit.varimp())\n",
    "\n",
    "# predict\n",
    "pred = fit.predict(data_hex)\n",
    "Yhat = pred['predict'].as_data_frame()\n",
    "Y = data_hex['SUM_SML_EV'].as_data_frame()\n",
    "tmp = pd.concat([Y, Yhat], axis=1)\n",
    "mape = pd.DataFrame({'smape_100':[cal_smape_100(tmp['SUM_SML_EV'], tmp['predict'])]})\n",
    "\n",
    "print(perform)\n",
    "print(mape)\n",
    "print(varimp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#=================================================================#\n",
    "# 교차 검증\n",
    "#=================================================================#\n",
    "# STN별 리스트 생성 \n",
    "STNList = DATA['STN_ID'].unique()\n",
    "\n",
    "# 결과리스트 초기화 \n",
    "perform = list()\n",
    "varimp = list()\n",
    "mape = list()\n",
    "\n",
    "# MODEL RUNNING\n",
    "for i in range(len(STNList)):\n",
    "    stn = STNList[i]\n",
    "    print(\"STN_ID : \", stn, \"...computing\")\n",
    "    train = data_hex[data_hex['STN_ID'] != stn, :]\n",
    "    valid = data_hex[data_hex['STN_ID'] == stn, :]\n",
    "    \n",
    "    m = H2ORandomForestEstimator(ntrees=50, max_depth=18, sample_rate = 0.3, mtries = -1, seed=1)\n",
    "    m.train(x = varList, y = y, training_frame = train)\n",
    "\n",
    "    perform_df = pd.DataFrame({'mse':[m.mse()], 'r2':[m.r2()]})\n",
    "    perform.append(perform_df)\n",
    "    \n",
    "    varimp_df = m.varimp(True)\n",
    "    varimp.append(varimp_df)    \n",
    "    \n",
    "    print(\"STN_ID : \", stn, \"...predicting\")\n",
    "    \n",
    "    pred = m.predict(valid)\n",
    "    Yhat = pred['predict'].as_data_frame()\n",
    "    Y = valid['SUM_SML_EV'].as_data_frame()\n",
    "    tmp = pd.concat([Y, Yhat], axis=1)\n",
    "    \n",
    "    mape_df = pd.DataFrame({'STN_ID':stn, 'smape_100':[cal_smape_100(tmp['SUM_SML_EV'], tmp['predict'])]}, index=[i])\n",
    "    mape.append(mape_df)\n",
    "    \n",
    "mape_result=pd.concat(mape)\n",
    "mape_result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
